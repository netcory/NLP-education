{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "# 강의 자료 18. ch7_최적화_GradientDescent\r\n",
    "\r\n",
    "gradient descent를 이용하면 loss 값을 최소화 하는 파라미터를 찾을 수 있다.\r\n",
    "\r\n",
    "global minima가 아니라 local minima (골짜기) 에 빠질 위험이 있다. 걱정할 필요는 없지만 이것에 대해 알고는 있어야 한다.\r\n",
    "\r\n",
    "learning rate의 의미: gradient의 상수배 만큼 이동하겠다. 파라미터가 업데이트 될 때 step-size를 정해주게 됨.\r\n",
    "\r\n",
    "learning rate는 어떻게 정할 것인가? 아주 작은 값(ex)0.0001)로 오래 돌리는 것도 괜찮음. 나중에 adam optimizer를 통해 learning rate에 대한 고민 해결 가능하다.\r\n",
    "\r\n",
    "07-05-pytorch_auto_grad 실습 파일\r\n",
    "\r\n",
    "07-06-grdient_descent 실습 파일\r\n",
    "\r\n",
    "# 강의 자료 19. ch.8_Linear Regression by Gradient Descent\r\n",
    "\r\n",
    "08-03-linear_regression_boston_housing 실습 파일\r\n",
    "\r\n",
    "딥러닝 필기시험 완료"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}