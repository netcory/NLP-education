{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "강의 자료 ch.12_하이퍼파라미터와 Adam\r\n",
    "\r\n",
    "모델 파라미터: 사용자가 조정하지 않음\r\n",
    "\r\n",
    "하이퍼 파라미터: 사용자가 조정함. 데이터로부터 자동으로 학습할 수 없지만, 모델의 성능에 영향을 끼치는 파라미터.\r\n",
    "\r\n",
    "결국 우리가 할 일은 하이퍼 파라미터를 튜닝하는 일\r\n",
    "\r\n",
    "하이퍼 파라미터의 종류: Network depth & width, Learning Rate, activation func, 초기화 방법, epoch 수, batch_size\r\n",
    "\r\n",
    "(optimizer Adam의 경우 learning rate를 자기가 알아서 찾기 때문에 편한 게 있음)\r\n",
    "\r\n",
    "1. Network Depth & Width\r\n",
    "\r\n",
    "깊으면 복잡한 함수를 학습할 수 있지만 너무 깊으면 overfitting의 원인이 되며 최적화가 어려움.\r\n",
    "\r\n",
    "너무 얕으면 복잡한 데이터의 관계 또는 함수를 배울 수 없음.\r\n",
    "\r\n",
    "Tuning을 통해 최적의 architecture를 찾아야 한다.\r\n",
    "\r\n",
    "2. Learning rate\r\n",
    "\r\n",
    "3. Other hyper-parameters...\r\n",
    "\r\n",
    "\r\n",
    "결국 하이퍼 파라미터는 다양한 실험을 통해 최적값을 찾아야 함.\r\n",
    "\r\n",
    "모든 파라미터가 다 중요하지 않음.\r\n",
    "\r\n",
    "critical 파라미터를 인지하고 주로 튜닝하는 것이 중요.\r\n",
    "\r\n",
    "Learning rate: 기울기에 따른 경사도 하강에서 파라미터 업데이트의 크기를 정하는 역할\r\n",
    "\r\n",
    "너무 큰 LR: loss가 발산할 수 있음\r\n",
    "\r\n",
    "너무 작은 LR: 수렴이 너무 늦음, 자칫 local minima에 빠질 수 있음\r\n",
    "\r\n",
    "학습 초반에는 큰 LR, 후반에는 작은 LR로 최적화\r\n",
    "\r\n",
    "12-05-adam(california_housing) 실습 파일\r\n",
    "\r\n",
    "강의자료 ch.13_overfitting\r\n",
    "\r\n",
    "테스트 셋을 공들여 만들어야 한다. 학습 데이터와 절대 겹치면 안된다.\r\n",
    "\r\n",
    "13-03-DNN_Regression_Train_Valid_Test 실습 파일\r\n",
    "\r\n",
    "강의자료 ch.14_DNN 분류\r\n",
    "\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}