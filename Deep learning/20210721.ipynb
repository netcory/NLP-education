{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "강의자료 ch.14_DNN 분류 이어서\r\n",
    "\r\n",
    "14-03-binary_classification 실습 파일\r\n",
    "\r\n",
    "강의자료 ch.14_DNN 분류에서 p.40 표는 외워야 함\r\n",
    "\r\n",
    "다중분류\r\n",
    "\r\n",
    "다중분류는 클래스 별 확률값들은 전부 더하면 1인 확률 문제로 치환할 수 있다.\r\n",
    "\r\n",
    "14-07-Mulit-classification(MNIST)_gpu (cpu 로 해도 같음) 실습 파일\r\n",
    "\r\n",
    "강의자료 ch.15_DNN_Regularizations\r\n",
    "\r\n",
    "Regularization이란? Overfitting을 최소화하기 위해 generalization error를 낮추기 위한 방법/알고리즘\r\n",
    "\r\n",
    "주의할 점: Regularization 과정에서 training error가 높아질 수 있다. Noise를 학습할 수도 있기 때문.\r\n",
    "\r\n",
    "다양한 Regularization 기법들\r\n",
    "\r\n",
    "1. Weight decay using L2 Norm\r\n",
    "\r\n",
    "보통 학습이 진행되면서 weight가 점점 커지는데 이를 L2 Norm 을 통해 방지.\r\n",
    "\r\n",
    "2. Data augmentation\r\n",
    "\r\n",
    "핵심 특징(feautre)를 간직한 상태로 noise를 더하여 데이터를 늘리는 방법\r\n",
    "\r\n",
    "3. Batch Normalization ** 가장 중요 **\r\n",
    "\r\n",
    "RNN에서는 사용 불가. RNN에서는 layer normalization을 사용한다.\r\n",
    "\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}